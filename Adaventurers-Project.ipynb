{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzySqtMD7PWW"
   },
   "source": [
    "<img src=\"https://www.epfl.ch/about/overview/wp-content/uploads/2020/07/logo-epfl-1024x576.png\" style=\"padding-right:10px;width:140px;float:left;padding10:3px;padding-top:10px;\"></td>\n",
    "<h1 style=\"white-space: nowrap\"> Project Milestone 3</h1>\n",
    "<h3 style=\"white-space: nowrap;margin:22px\">  Applied Data Analysis (CS-401) </h3>\n",
    "<hr style=\"clear:both\">\n",
    "<p style=\"font-size:1em; margin:2px; text-align:justify\">\n",
    "    <b>Description: </b>This Colab notebook is aimed at investigating the QuoteBank database provided in the course of Applied Data Analysis by Professor Robert West. The data consists of quotes from newspapers dated from 2015 to 2020 included.\n",
    "</p>\n",
    "<p style=\"font-size:0.95em; margin:0px\"><b>Authors: </b> \n",
    "    <a href=\"mailto:lucas.brunschwig@epfl.ch\"> Lucas Brunschwig</a>, \n",
    "    <a href=\"mailto:ioannis.mavrothalassitis@epfl.ch\"> Ioannis Mavrothalassitis</a>, \n",
    "    <a href=\"mailto:axelle.piguet@epfl.ch\"> Axelle Piguet</a>, \n",
    "    <a href=\"mailto:ester.simkova@epfl.ch\"> Ester Simkova</a>\n",
    "\n",
    "</p>\n",
    "<p style=\"font-size:0.95em; margin:0px\"><b>Data: </b> </p>\n",
    "<ul> \n",
    "   <li> Raw data: <a href=\"https://zenodo.org/record/4277311#.YY0c_2DMJPY\">here</a>. </li>\n",
    "   <li> Processed Data: <a href=\"https://zenodo.org/record/4277311#.YY0c_2DMJPY\">here</a> </li>\n",
    "</ul>\n",
    "\n",
    "<p  style=\"font-size:0.95em; margin:0px\">\n",
    "    <b>Last modification</b>: 16.12.2021 \n",
    "</p>\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NwCArqbE9N8x"
   },
   "source": [
    "\n",
    "<b><u>Table of contents</u></b>\n",
    "\n",
    "0. [Loading](#0)\n",
    "    \n",
    "    0.1. [Installation](#0-1)\n",
    "    \n",
    "    0.2. [Libraries](#0-2)\n",
    "    \n",
    "    0.3. [Preliminary Data Exploration](#0-3)\n",
    "\n",
    "\n",
    "\n",
    "1. [Bertopic Modeling](#1)  \n",
    "    \n",
    "    1.1. [Parameters](#1-1)\n",
    "    \n",
    "    1.2. [Data Loading](#1-2)\n",
    "    \n",
    "    1.3. [Bertopic Analysis](#1-3)\n",
    "\n",
    "\n",
    "2. [Topics Analysis](#2)\n",
    "\n",
    "    2.1 [Topics Selection](#1-1)\n",
    "    \n",
    "    2.2 [Visualisation over time](#1-2) \n",
    "    \n",
    "    2.3 Removal of common english words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Bf62l4QRfQr"
   },
   "source": [
    "## 0. Loading <a class=\"anchor\" id=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the necessary libraries to run the notebook. One of the libraries necessit a specific version of some common libraries and you will have to restart the environment once installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Installation <a class=\"anchor\" id=\"0-1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two libraries are necessary to install the most exotic libraries to extract the topics of each quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Using cached sentence-transformers-2.1.0.tar.gz (78 kB)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.14.1-py3-none-any.whl (3.4 MB)\n",
      "Collecting tokenizers>=0.10.3\n",
      "  Downloading tokenizers-0.10.3-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.59.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.10.0)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.11.2-cp38-cp38-win_amd64.whl (985 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.20.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from sentence_transformers) (0.24.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.6.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from sentence_transformers) (3.6.1)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "Collecting huggingface-hub\n",
      "  Using cached huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (20.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.4.4)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.4.7)\n",
      "Requirement already satisfied: click in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from nltk->sentence_transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from nltk->sentence_transformers) (1.0.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.10)\n",
      "Requirement already satisfied: six in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from scikit-learn->sentence_transformers) (2.1.0)\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-1.10.1-cp38-cp38-win_amd64.whl (226.6 MB)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from torchvision->sentence_transformers) (8.2.0)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.1.0-py3-none-any.whl size=120999 sha256=8a3232e3a5cb49bb5f21f9fde9d497f3ef7de9b0648f5e04e158e25df07cef16\n",
      "  Stored in directory: c:\\users\\lucas\\appdata\\local\\pip\\cache\\wheels\\52\\19\\88\\6625593382e23a926740e6fcee0f2df0a0de25766094842a28\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: torch, tokenizers, sacremoses, huggingface-hub, transformers, torchvision, sentencepiece, sentence-transformers\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.10.0\n",
      "    Uninstalling torch-1.10.0:\n",
      "      Successfully uninstalled torch-1.10.0\n",
      "Successfully installed huggingface-hub-0.2.1 sacremoses-0.0.46 sentence-transformers-2.1.0 sentencepiece-0.1.96 tokenizers-0.10.3 torch-1.10.1 torchvision-0.11.2 transformers-4.14.1\n",
      "Collecting bertopic\n",
      "  Downloading bertopic-0.9.4-py2.py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from bertopic) (4.59.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from bertopic) (0.24.1)\n",
      "Requirement already satisfied: plotly>=4.7.0 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from bertopic) (5.3.1)\n",
      "Requirement already satisfied: pyyaml<6.0 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from bertopic) (5.4.1)\n",
      "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from bertopic) (1.2.4)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from bertopic) (1.20.1)\n",
      "Requirement already satisfied: sentence-transformers>=0.4.1 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from bertopic) (2.1.0)\n",
      "Collecting umap-learn>=0.5.0\n",
      "  Using cached umap-learn-0.5.2.tar.gz (86 kB)\n",
      "Collecting hdbscan>=0.8.27\n",
      "  Using cached hdbscan-0.8.27.tar.gz (6.4 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.27->bertopic) (1.15.0)\n",
      "Requirement already satisfied: cython>=0.27 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.27->bertopic) (0.29.23)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.27->bertopic) (1.0.1)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.27->bertopic) (1.6.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2.8.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from plotly>=4.7.0->bertopic) (8.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2.post1->bertopic) (2.1.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.1.96)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (1.10.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (3.6.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (4.14.1)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.2.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.11.2)\n",
      "Requirement already satisfied: tokenizers>=0.10.3 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.10.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.7.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2021.4.4)\n",
      "Requirement already satisfied: requests in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (20.9)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.0.46)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.4.7)\n",
      "Requirement already satisfied: numba>=0.49 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.53.1)\n",
      "Collecting pynndescent>=0.5\n",
      "  Using cached pynndescent-0.5.5.tar.gz (1.1 MB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (52.0.0.post20210125)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.36.0)\n",
      "Requirement already satisfied: click in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (7.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (4.0.0)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (8.2.0)\n",
      "Building wheels for collected packages: hdbscan, umap-learn, pynndescent\n",
      "  Building wheel for hdbscan (PEP 517): started\n",
      "  Building wheel for hdbscan (PEP 517): finished with status 'done'\n",
      "  Created wheel for hdbscan: filename=hdbscan-0.8.27-cp38-cp38-win_amd64.whl size=650895 sha256=97d9e1b84068bf4e12f89b4deedba22421a628600ea7bbe67c537fa06c912f54\n",
      "  Stored in directory: c:\\users\\lucas\\appdata\\local\\pip\\cache\\wheels\\26\\f2\\c2\\eab587fff76dc9ffc9a9bf3ca0e44e26d2ef6425264492df65\n",
      "  Building wheel for umap-learn (setup.py): started\n",
      "  Building wheel for umap-learn (setup.py): finished with status 'done'\n",
      "  Created wheel for umap-learn: filename=umap_learn-0.5.2-py3-none-any.whl size=82696 sha256=3fe9bc5dc6a83ffb620e4829d2ac1b4c8caaef3d3258757dc46a1f95d93cca35\n",
      "  Stored in directory: c:\\users\\lucas\\appdata\\local\\pip\\cache\\wheels\\f2\\64\\75\\df601da9514261c8cb0830b9515d2b94b5a51f09ddeae92b9e\n",
      "  Building wheel for pynndescent (setup.py): started\n",
      "  Building wheel for pynndescent (setup.py): finished with status 'done'\n",
      "  Created wheel for pynndescent: filename=pynndescent-0.5.5-py3-none-any.whl size=52587 sha256=69b6887110c4182dfba1ac5cae77935ad971b13e2aa5da93de99a787a6a652a1\n",
      "  Stored in directory: c:\\users\\lucas\\appdata\\local\\pip\\cache\\wheels\\e4\\0e\\b5\\07c0c231aacb04e5d1046fe7459bb27ea79f95b5edbe88e435\n",
      "Successfully built hdbscan umap-learn pynndescent\n",
      "Installing collected packages: pynndescent, umap-learn, hdbscan, bertopic\n",
      "Successfully installed bertopic-0.9.4 hdbscan-0.8.27 pynndescent-0.5.5 umap-learn-0.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install  sentence_transformers\n",
    "!pip install bertopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBsllopZ-D7b"
   },
   "source": [
    "### 0.2 Libraries <a class=\"anchor\" id=\"0-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CKIX5vW0SCAA",
    "outputId": "ce9b734f-9580-495d-c349-8b62022449f0"
   },
   "outputs": [],
   "source": [
    "##################### IMPORTS #####################\n",
    "\n",
    "# NumPy to use arrays\n",
    "import numpy as np\n",
    "\n",
    "# DataFrame handling\n",
    "import pandas as pd\n",
    "\n",
    "# Data Decompression\n",
    "import bz2\n",
    "\n",
    "# Data Structure\n",
    "import json\n",
    "\n",
    "# Useful for chronological ordering\n",
    "from datetime import datetime, date ,time \n",
    "import string\n",
    "\n",
    "\n",
    "import os\n",
    "from os import walk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Preliminary Data Exploration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just some preliminary exploration of the quotes that we have in the dataframe such that we get the global idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Format of a QUOTE</b>\n",
    "\n",
    "Here, we will have a look at the shape of the data. The format of the original data-base is as follows:\n",
    "\n",
    "* quoteID: a unique index that allow to select a quote\n",
    "* quotation: the quote from beginning to end\n",
    "* speakers: the most probable speakers (None, if unsure)\n",
    "* qids: not sure\n",
    "* date: date of parution to the miliseconds\n",
    "* numOccurrences: the number of articles where the quote was mentionned\n",
    "* probas: probability that different speakers said the quote\n",
    "* urls: urls of articles that mentionned this quote\n",
    "* phase: when was the data retrieved (not of interest to us)\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading file: \"data/quotes-2015.json.bz2\"\n",
      "Beginning: Loading Quotes...\n",
      "    Loading... 0.00 %\n",
      "    Loading... 11.11 %\n",
      "    Loading... 22.22 %\n",
      "    Loading... 33.33 %\n",
      "    Loading... 44.44 %\n",
      "    Loading... 55.56 %\n",
      "    Loading... 66.67 %\n",
      "    Loading... 77.78 %\n",
      "    Loading... 88.89 %\n",
      "    Loading... 100.00 %\n",
      "Number of quotes loaded in data/quotes-2015.json.bz2: 100\n"
     ]
    }
   ],
   "source": [
    "# Load Data \n",
    "from data_loader import data_loader\n",
    "\n",
    "files_to_load = \"quotes-2015.json.bz2\"\n",
    "limits=10\n",
    "chunk_ = 10\n",
    "df_quotes = data_loader(\"data/\"+files_to_load, limit = limits-1, chunksize_ = chunk_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape (rows, columns): (100, 9)\n",
      "Column names as depicted above: Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>probas</th>\n",
       "      <th>urls</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-08-31-000271</td>\n",
       "      <td>... a great day for veterans here in Littleton...</td>\n",
       "      <td>Jeanne Shaheen</td>\n",
       "      <td>[Q270316]</td>\n",
       "      <td>2015-08-31 02:10:00</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Jeanne Shaheen, 0.742], [None, 0.2359], [Kel...</td>\n",
       "      <td>[http://www.unionleader.com/article/20150831/N...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-12-08-029916</td>\n",
       "      <td>How FFA scored 32 own goals in 18 months and C...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>2015-12-08 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>[[None, 0.563], [David Gallop, 0.437]]</td>\n",
       "      <td>[http://feeds.theroar.com.au/~r/theroar/~3/tZ3...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-09-10-000206</td>\n",
       "      <td>[ Amy ] was placed under an unacceptable amoun...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>2015-09-10 21:18:59</td>\n",
       "      <td>1</td>\n",
       "      <td>[[None, 0.9634], [Amy Robinson, 0.0366]]</td>\n",
       "      <td>[http://www.thefashionspot.com/buzz-news/lates...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-07-23-032707</td>\n",
       "      <td>How High Will These Numbers Go?</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>2015-07-23 14:57:49</td>\n",
       "      <td>3</td>\n",
       "      <td>[[None, 0.9019], [Chubby Checker, 0.0981]]</td>\n",
       "      <td>[http://www.billboard.com/node/6641719, http:/...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             quoteID                                          quotation  \\\n",
       "0  2015-08-31-000271  ... a great day for veterans here in Littleton...   \n",
       "1  2015-12-08-029916  How FFA scored 32 own goals in 18 months and C...   \n",
       "2  2015-09-10-000206  [ Amy ] was placed under an unacceptable amoun...   \n",
       "3  2015-07-23-032707                    How High Will These Numbers Go?   \n",
       "\n",
       "          speaker       qids                date  numOccurrences  \\\n",
       "0  Jeanne Shaheen  [Q270316] 2015-08-31 02:10:00               2   \n",
       "1            None         [] 2015-12-08 00:00:00               2   \n",
       "2            None         [] 2015-09-10 21:18:59               1   \n",
       "3            None         [] 2015-07-23 14:57:49               3   \n",
       "\n",
       "                                              probas  \\\n",
       "0  [[Jeanne Shaheen, 0.742], [None, 0.2359], [Kel...   \n",
       "1             [[None, 0.563], [David Gallop, 0.437]]   \n",
       "2           [[None, 0.9634], [Amy Robinson, 0.0366]]   \n",
       "3         [[None, 0.9019], [Chubby Checker, 0.0981]]   \n",
       "\n",
       "                                                urls phase  \n",
       "0  [http://www.unionleader.com/article/20150831/N...     E  \n",
       "1  [http://feeds.theroar.com.au/~r/theroar/~3/tZ3...     E  \n",
       "2  [http://www.thefashionspot.com/buzz-news/lates...     E  \n",
       "3  [http://www.billboard.com/node/6641719, http:/...     E  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Data Shape (rows, columns):\", np.shape(df_quotes))\n",
    "print(\"Column names as depicted above:\", df_quotes.columns)\n",
    "df_quotes.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One typical event:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "quoteID                                           2015-08-31-000271\n",
       "quotation         ... a great day for veterans here in Littleton...\n",
       "speaker                                              Jeanne Shaheen\n",
       "qids                                                      [Q270316]\n",
       "date                                            2015-08-31 02:10:00\n",
       "numOccurrences                                                    2\n",
       "probas            [[Jeanne Shaheen, 0.742], [None, 0.2359], [Kel...\n",
       "urls              [http://www.unionleader.com/article/20150831/N...\n",
       "phase                                                             E\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"One typical event:\")\n",
    "print()\n",
    "df_quotes.loc[0] # example of one row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Search for a specific event and see if the quotes relate to it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 9)\n"
     ]
    }
   ],
   "source": [
    "df_quotes.sort_values(by=['date'], inplace=True, ascending=True)\n",
    "\n",
    "##### IMPORTANT: we take only a subset of quotes for a given time window\n",
    "df_quotes = df_quotes[df_quotes.date.dt.month == 11] # The 2016 United States elections were held on Tuesday, November 8, 2016, let's look around this period\n",
    "df_quotes = df_quotes[df_quotes.date.dt.day <= 12]\n",
    "df_quotes = df_quotes[df_quotes.date.dt.day >= 5]\n",
    "df_quotes.head()\n",
    "print(np.shape(df_quotes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Pv2BlwTVU1T"
   },
   "source": [
    "## 1. Bertopic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load two python files which delocates the code and eases the reading of the notebook. \n",
    "* data_loader.py contains all the necessary functions such that we can load data from different functions. In order to process the data without overloading the ram and gpu to our disposition, we decided to limit the quotes of each month at 100'000 quotes. This choice was justified due to the facts that we found relevant topics in the results.\n",
    "* bert.py contains all the necessary functions to perform the analysis by year and then by month. Each month is performed individually so that we can extract hot topics.\n",
    "\n",
    "Limitations and Considerations:\n",
    "* We don't use the whole dataset but this is not necessary as we only need to extract the hot topics. The sample that we extract is sufficient to significantly represent the topics that are of interest to us.\n",
    "* The month sliding window is significantly large but given that we are interested in major topics we can estimate that it will be discussed over at least one month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: C:\\Users\\Lucas\\OneDrive\\Bureau\\ada-2021-project-adaventurers\n",
      "\n",
      "Path results/bertopic/ already exists\n",
      "Path data/ already exists\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Directory\n",
    "\n",
    "path = os.getcwd()\n",
    "print(\"Current Directory:\",path)\n",
    "print()\n",
    "\n",
    "# Path needed to be ran\n",
    "paths = [\"results/bertopic/\",\"data/\"]\n",
    "\n",
    "for path in paths:\n",
    "    \n",
    "    # Check if necessary paths exists\n",
    "    if os.path.isdir(path): \n",
    "        print(\"Path %s already exists\" % path)\n",
    "        \n",
    "    else:\n",
    "        try:\n",
    "            os.mkdir(path)\n",
    "        except OSError:\n",
    "            print (\"Creation of the directory \\\"%s\\\" failed\" % path)\n",
    "\n",
    "        else:\n",
    "            print (\"Successfully created the directory %s \" % path)\n",
    "            if path==\"data\":\n",
    "                print(\"You will need to dowload the data of the Quotebank dataset to load them\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OULHJp9nSAM3",
    "outputId": "5b856642-b22a-4cd1-d808-d108a9c4c63e"
   },
   "outputs": [],
   "source": [
    "from data_loader import data_loader, clean_data\n",
    "from bert import get_year_topics\n",
    "\n",
    "# Parameters\n",
    "\n",
    "year = 2020 # which year do you want to analyze\n",
    "limits = 10 # the number of chunk to extract\n",
    "chunk_ = 10 # the number of quotes for each chunk --> 10*100'000 = 1M quotes in total\n",
    "threshold = 100000 # maximum number of quotes for one month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OULHJp9nSAM3",
    "outputId": "5b856642-b22a-4cd1-d808-d108a9c4c63e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beginning: Loading File\n",
      "\n",
      "\n",
      "Loading file: \"data/quotes-2020.json.bz2\"\n",
      "Beginning: Loading Quotes...\n",
      "    Loading... 0.00 %\n",
      "    Loading... 11.11 %\n",
      "    Loading... 22.22 %\n",
      "    Loading... 33.33 %\n",
      "    Loading... 44.44 %\n",
      "    Loading... 55.56 %\n",
      "    Loading... 66.67 %\n",
      "    Loading... 77.78 %\n",
      "    Loading... 88.89 %\n",
      "    Loading... 100.00 %\n",
      "Number of quotes loaded in data/quotes-2020.json.bz2: 100\n",
      "Done: Loading File\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files_to_load = f\"quotes-{year}.json.bz2\"\n",
    "data_year = pd.DataFrame()\n",
    "\n",
    "print(\"\\nBeginning: Loading File\\n\")\n",
    "\n",
    "data_year = data_loader(\"data/\"+files_to_load, limit = limits-1, chunksize_ = chunk_,thrs = threshold)\n",
    "\n",
    "print(\"Done: Loading File\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bert Topic Analysis:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OULHJp9nSAM3",
    "outputId": "5b856642-b22a-4cd1-d808-d108a9c4c63e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beginning: Bert Topic Analysis\n",
      "\n",
      "Year analysis:\n",
      "    month: 1, number of quotes: 33\n",
      "    month: 2, number of quotes: 30\n",
      "    month: 3, number of quotes: 24\n",
      "    month: 4, number of quotes: 13\n",
      "    month: 5, number of quotes: 0\n",
      "    month: 6, number of quotes: 0\n",
      "    month: 7, number of quotes: 0\n",
      "    month: 8, number of quotes: 0\n",
      "    month: 9, number of quotes: 0\n",
      "    month: 10, number of quotes: 0\n",
      "    month: 11, number of quotes: 0\n",
      "    month: 12, number of quotes: 0\n",
      "\n",
      "Example of the format\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase', 'topic_number', 'topic_prob'],\n",
      "      dtype='object')\n",
      "              quoteID                                          quotation  \\\n",
      "37  2020-01-03-001237  A lot of Americans [ were ] upset that this wa...   \n",
      "45  2020-01-03-001453    a methodical combination of planning and chance   \n",
      "23  2020-01-05-000384  A 25-year-old Sikh man, was killed by unidenti...   \n",
      "51  2020-01-07-001821                        a right-wing talking point.   \n",
      "16  2020-01-08-000594                   11 straight weeks of pre-season,   \n",
      "\n",
      "           speaker                                        qids  \\\n",
      "37   Julián Castro                                   [Q970720]   \n",
      "45            None                                          []   \n",
      "23   Harmeet Singh  [Q16225529, Q16228362, Q210955, Q42158060]   \n",
      "51   Ricky Gervais                                    [Q23517]   \n",
      "16  Aphelele Fassi                                 [Q56255401]   \n",
      "\n",
      "                  date  numOccurrences  \\\n",
      "37 2020-01-03 00:00:00               1   \n",
      "45 2020-01-03 23:11:40               1   \n",
      "23 2020-01-05 18:29:36               1   \n",
      "51 2020-01-07 00:00:00               1   \n",
      "16 2020-01-08 10:22:11               1   \n",
      "\n",
      "                                               probas  \\\n",
      "37  [[Julián Castro, 0.4781], [None, 0.4384], [Dav...   \n",
      "45             [[None, 0.7811], [Georg Nees, 0.2189]]   \n",
      "23  [[Harmeet Singh, 0.5227], [None, 0.4229], [Par...   \n",
      "51  [[Ricky Gervais, 0.6196], [None, 0.2932], [Mar...   \n",
      "16         [[Aphelele Fassi, 0.9033], [None, 0.0967]]   \n",
      "\n",
      "                                                 urls phase  topic_number  \\\n",
      "37  [http://keranews.org/post/juli-n-castro-ends-2...     E            -1   \n",
      "45  [https://www.artnews.com/art-in-america/featur...     E            -1   \n",
      "23  [http://www.nagalandpost.com/after-nankana-att...     E            -1   \n",
      "51  [http://www.neonnettle.com/news/9888-ricky-ger...     E            -1   \n",
      "16  [https://www.sport24.co.za/Rugby/SuperRugby/sh...     E            -1   \n",
      "\n",
      "    topic_prob  \n",
      "37         0.0  \n",
      "45         0.0  \n",
      "23         0.0  \n",
      "51         0.0  \n",
      "16         0.0  \n",
      "\n",
      "Index(['Topic', 'Count', 'Name', 'Words', 'Month'], dtype='object')\n",
      "  Topic Count              Name  Words Month\n",
      "0    -1    33  -1_the_of_to_and  False     1\n",
      "1     0    30  -1_the_and_of_to  False     2\n",
      "2     1    24  -1_the_and_to_of  False     3\n",
      "3     2    13  -1_the_and_to_is  False     4\n",
      "\n",
      "Done: Bert Topic Analysis\n",
      "\n",
      "Saving Data:\n",
      "Done savings Data\n",
      "\n",
      "You can find the results in \"results/bertopic/\" folder\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nBeginning: Bert Topic Analysis\\n\")\n",
    "\n",
    "topic_assignation, prob_assignation, topic_list= get_year_topics(data_year)\n",
    "\n",
    "topics_assignation = [element  for month in topic_assignation  for element in month ]\n",
    "prob_assignation = [element  for month in prob_assignation  for element in month ]\n",
    "\n",
    "data_year.sort_values(by=['date'], inplace=True, ascending=True)\n",
    "\n",
    "data_year[\"topic_number\"]=topics_assignation\n",
    "data_year[\"topic_prob\"]=prob_assignation\n",
    "\n",
    "\n",
    "print(\"\\nExample of the format\")\n",
    "print(data_year.columns)\n",
    "print(data_year.head())\n",
    "print()\n",
    "print(topic_list.columns)\n",
    "print(topic_list.head())\n",
    "\n",
    "print(\"\\nDone: Bert Topic Analysis\\n\")\n",
    "\n",
    "#--------------------------------------------------------#\n",
    "# Saving Topics\n",
    "\n",
    "print(\"Saving Data:\")\n",
    "#data_year.to_pickle(f\"results/bertopic/{year}_quotes_with_topics_{len(data_year)}.pkl\")\n",
    "#topic_list.to_pickle(f\"results/bertopic/{year}topics_by_month_{len(data_year)}.pkl\")\n",
    "print(\"Done savings Data\")\n",
    "\n",
    "#--------------------------------------------------------#\n",
    "# End of topic extractions\n",
    "print(\"\\nYou can find the results in \\\"results/bertopic/\\\" folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Topics Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we extracted topics of each month and associate each quotes to a topic we will discuss here the results that we got. The goal is to extract the main topics that are specific to one event and represent it over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List of the Bertopic Results that exists**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have a previous pandas version here it will upload it for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==1.3.1 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from pandas==1.3.1) (1.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from pandas==1.3.1) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from pandas==1.3.1) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lucas\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas==1.3.1) (1.15.0)\n",
      "1.3.1\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "!pip install pandas==1.3.1\n",
    "import pandas as pd\n",
    "importlib.reload(pd)\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Topics Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0,1 | 2015_quotes_with_topics_1000000.pkl 2015_topics_by_month_1000000.pkl\n",
      "Index: 2,3 | 2016topics_by_month_737263.pkl 2016_quotes_with_topics_737263.pkl\n",
      "Index: 4,5 | 2020topics_by_month_400000.pkl 2020_quotes_with_topics_400000.pkl\n"
     ]
    }
   ],
   "source": [
    "mypath=\"results/bertopic/\"\n",
    "f = []\n",
    "for (dirpath, dirnames, filenames) in walk(mypath):\n",
    "    f.extend(filenames)\n",
    "    break\n",
    "for i in range(0,len(f),2):\n",
    "    print(f\"Index: {i},{i+1} |\", f[i],f[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the file of interest\n",
    "i = 0\n",
    "quote_file = f[i]\n",
    "topic_file = f[i+1]\n",
    "\n",
    "# Load results files\n",
    "df_quotes = pd.read_pickle(\"results/bertopic/\"+quote_file)\n",
    "df_topics = pd.read_pickle(\"results/bertopic/\"+topic_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Topic', 'Count', 'Name', 'Words', 'Month'], dtype='object')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------#\n",
    "# Topics overview by months\n",
    "\n",
    "# Format of the topic dataFrame\n",
    "print(df_topics.columns)\n",
    "print()\n",
    "year = 2015\n",
    "\n",
    "open(f\"results/top_topics/{year}_top_topics.txt\", 'w').close()\n",
    "f = open(f\"results/top_topics/{year}_top_topics.txt\", \"a\")\n",
    "\n",
    "# Selection of a specific month\n",
    "for month in range(1,13):\n",
    "    hot_topics = df_topics[df_topics.Month==month].sort_values(by=\"Count\",ascending=False)\n",
    "    f.write(\"#------------------------------------# \\n\")\n",
    "    f.write(f\"Month {month} \\n\\n\")\n",
    "    for topic_selected in hot_topics[1:10].Topic.values:\n",
    "\n",
    "        # Quotes associate to the top topic\n",
    "        # Will be change to target most probable quotes\n",
    "        #print(df_quotes.columns)\n",
    "        quotes_ = df_quotes[(df_quotes.date.dt.month ==month) & (df_quotes.topic_number==topic_selected)].sort_values(by=\"topic_prob\",ascending=False).quotation.values\n",
    "\n",
    "        f.write(f\"Words associated to topic {topic_selected}: \\n\")\n",
    "        f.write(str(hot_topics[hot_topics.Topic==topic_selected].Words.values))\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.write(\"Top quotes associated to the same topic: \\n\")\n",
    "        \n",
    "        for i,quote in enumerate(quotes_[0:5]):\n",
    "            f.write(f\"quote {i}: \")\n",
    "            f.write(quote)\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\\n\")\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Common Topic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on a systematic analysis of the top 20 topics. We selected the words that are the most common and discard them so that we can achieve more interesting topics as the top topics. We also use it to visualize the common topics and their evolution over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Topics of Interest Extraction"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ADA_M2_datahandling.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
